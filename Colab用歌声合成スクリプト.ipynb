{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "yui2024式 歌唱合成ソフト v9.0 — Google Colab 完全単体版\n",
        "==========================================================\n",
        "kana_map.py / ut_one.py をこのファイルに完全に内包しています。\n",
        "アップロード不要。このファイル1つをセルに貼り付けるか\n",
        "%run で実行するだけで動作します。\n",
        "\n",
        "【実行方法】\n",
        "  このファイルを Colab にアップロードして:\n",
        "      %run singing_synth_colab.py\n",
        "  または中身をそのままセルに貼り付けて実行\n",
        "\n",
        "【モデルファイルの読み込み方法 (どちらか)】\n",
        "  A) Google Drive をマウントしてパスを入力:\n",
        "       from google.colab import drive; drive.mount('/content/drive')\n",
        "       パス例: /content/drive/MyDrive/model.yvs\n",
        "\n",
        "  B) Colab のサイドバーからモデルファイルをアップロード:\n",
        "       パス例: /content/model.yvs\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1. インストール\n",
        "# =============================================================================\n",
        "import subprocess, sys\n",
        "for _pkg in [\"gradio\", \"pyworld\", \"librosa\", \"soundfile\", \"scipy\"]:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", _pkg])\n",
        "\n",
        "# =============================================================================\n",
        "# 2. インポート\n",
        "# =============================================================================\n",
        "import os, tempfile, traceback, pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import pyworld as pw\n",
        "from scipy import signal\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal import medfilt\n",
        "from scipy.interpolate import interp1d\n",
        "import gradio as gr\n",
        "\n",
        "try:\n",
        "    from torch.nn.utils.parametrizations import weight_norm\n",
        "except ImportError:\n",
        "    from torch.nn.utils import weight_norm\n",
        "# =============================================================================\n",
        "# ★ モデル登録テーブル ★\n",
        "# =============================================================================\n",
        "# ここにモデルを登録してください。\n",
        "# 形式: \"表示名\": \"Google Drive 上のフルパス (.yvs / .pth)\"\n",
        "#\n",
        "# 【追加方法】\n",
        "#   1. このテーブルに新しい行を 1 行追加するだけです。\n",
        "#   2. 例:\n",
        "#        \"歌手名A\": \"/content/drive/MyDrive/models/singer_a.yvs\",\n",
        "#\n",
        "# ⚠ Google Drive を使う場合は事前に以下をセルで実行してください:\n",
        "#      from google.colab import drive; drive.mount(\"/content/drive\")\n",
        "# =============================================================================\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    # ↓ ここから登録 ↓ ------------------------------------------------\n",
        "    \"yui2024\": \"/content/drive/MyDrive/model/yui.yvs\",\n",
        "    \"波音リツ\": \"/content/drive/MyDrive/model/base_model.yvs\",\n",
        "    # ↑ ここまで登録 ↑ ------------------------------------------------\n",
        "}\n",
        "\n",
        "# ドロップダウン用の選択肢リスト（自動生成・編集不要）\n",
        "_MODEL_CHOICES = list(MODEL_REGISTRY.keys())\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3. kana_map.py — 完全内包 (ファイルそのまま)\n",
        "# =============================================================================\n",
        "# Manual Kana-to-Phoneme Mapping for Singing Voice Synthesis\n",
        "# Matches pyopenjtalk output format where possible.\n",
        "# v9.0: Fixed mixed-language keys\n",
        "\n",
        "KANA_MAPPING = {\n",
        "    # Basic Vowels\n",
        "    'あ': ['a'], 'い': ['i'], 'う': ['u'], 'え': ['e'], 'お': ['o'],\n",
        "    'ア': ['a'], 'イ': ['i'], 'ウ': ['u'], 'エ': ['e'], 'オ': ['o'],\n",
        "\n",
        "    # K-line\n",
        "    'か': ['k', 'a'], 'き': ['k', 'i'], 'く': ['k', 'u'], 'け': ['k', 'e'], 'こ': ['k', 'o'],\n",
        "    'カ': ['k', 'a'], 'キ': ['k', 'i'], 'ク': ['k', 'u'], 'ケ': ['k', 'e'], 'コ': ['k', 'o'],\n",
        "\n",
        "    # S-line\n",
        "    'さ': ['s', 'a'], 'し': ['sh', 'i'], 'す': ['s', 'u'], 'せ': ['s', 'e'], 'そ': ['s', 'o'],\n",
        "    'サ': ['s', 'a'], 'シ': ['sh', 'i'], 'ス': ['s', 'u'], 'セ': ['s', 'e'], 'ソ': ['s', 'o'],\n",
        "\n",
        "    # T-line\n",
        "    'た': ['t', 'a'], 'ち': ['ch', 'i'], 'つ': ['ts', 'u'], 'て': ['t', 'e'], 'と': ['t', 'o'],\n",
        "    'タ': ['t', 'a'], 'チ': ['ch', 'i'], 'ツ': ['ts', 'u'], 'テ': ['t', 'e'], 'ト': ['t', 'o'],\n",
        "\n",
        "    # N-line\n",
        "    'な': ['n', 'a'], 'に': ['n', 'i'], 'ぬ': ['n', 'u'], 'ね': ['n', 'e'], 'の': ['n', 'o'],\n",
        "    'ナ': ['n', 'a'], 'ニ': ['n', 'i'], 'ヌ': ['n', 'u'], 'ネ': ['n', 'e'], 'ノ': ['n', 'o'],\n",
        "\n",
        "    # H-line\n",
        "    'は': ['h', 'a'], 'ひ': ['h', 'i'], 'ふ': ['f', 'u'], 'へ': ['h', 'e'], 'ほ': ['h', 'o'],\n",
        "    'ハ': ['h', 'a'], 'ヒ': ['h', 'i'], 'フ': ['f', 'u'], 'ヘ': ['h', 'e'], 'ホ': ['h', 'o'],\n",
        "\n",
        "    # M-line\n",
        "    'ま': ['m', 'a'], 'み': ['m', 'i'], 'む': ['m', 'u'], 'め': ['m', 'e'], 'も': ['m', 'o'],\n",
        "    'マ': ['m', 'a'], 'ミ': ['m', 'i'], 'ム': ['m', 'u'], 'メ': ['m', 'e'], 'モ': ['m', 'o'],\n",
        "\n",
        "    # Y-line\n",
        "    'や': ['y', 'a'], 'ゆ': ['y', 'u'], 'よ': ['y', 'o'],\n",
        "    'ヤ': ['y', 'a'], 'ユ': ['y', 'u'], 'ヨ': ['y', 'o'],\n",
        "\n",
        "    # R-line\n",
        "    'ら': ['r', 'a'], 'り': ['r', 'i'], 'る': ['r', 'u'], 'れ': ['r', 'e'], 'ろ': ['r', 'o'],\n",
        "    'ラ': ['r', 'a'], 'リ': ['r', 'i'], 'ル': ['r', 'u'], 'レ': ['r', 'e'], 'ロ': ['r', 'o'],\n",
        "\n",
        "    # W-line\n",
        "    'わ': ['w', 'a'], 'を': ['o'], 'ん': ['N'],\n",
        "    'ワ': ['w', 'a'], 'ヲ': ['o'], 'ン': ['N'],\n",
        "\n",
        "    # G-line (Dakuten)\n",
        "    'が': ['g', 'a'], 'ぎ': ['g', 'i'], 'ぐ': ['g', 'u'], 'げ': ['g', 'e'], 'ご': ['g', 'o'],\n",
        "    'ガ': ['g', 'a'], 'ギ': ['g', 'i'], 'グ': ['g', 'u'], 'ゲ': ['g', 'e'], 'ゴ': ['g', 'o'],\n",
        "\n",
        "    # Z-line (Dakuten)\n",
        "    'ざ': ['z', 'a'], 'じ': ['j', 'i'], 'ず': ['z', 'u'], 'ぜ': ['z', 'e'], 'ぞ': ['z', 'o'],\n",
        "    'ザ': ['z', 'a'], 'ジ': ['j', 'i'], 'ズ': ['z', 'u'], 'ゼ': ['z', 'e'], 'ゾ': ['z', 'o'],\n",
        "    'じぇ': ['j', 'e'], 'ジェ': ['j', 'e'],\n",
        "\n",
        "    # D-line (Dakuten)\n",
        "    'だ': ['d', 'a'], 'ぢ': ['j', 'i'], 'づ': ['z', 'u'], 'で': ['d', 'e'], 'ど': ['d', 'o'],\n",
        "    'ダ': ['d', 'a'], 'ヂ': ['j', 'i'], 'ヅ': ['z', 'u'], 'デ': ['d', 'e'], 'ド': ['d', 'o'],\n",
        "\n",
        "    # B-line (Dakuten)\n",
        "    'ば': ['b', 'a'], 'び': ['b', 'i'], 'ぶ': ['b', 'u'], 'べ': ['b', 'e'], 'ぼ': ['b', 'o'],\n",
        "    'バ': ['b', 'a'], 'ビ': ['b', 'i'], 'ブ': ['b', 'u'], 'ベ': ['b', 'e'], 'ボ': ['b', 'o'],\n",
        "\n",
        "    # P-line (Handakuten)\n",
        "    'ぱ': ['p', 'a'], 'ぴ': ['p', 'i'], 'ぷ': ['p', 'u'], 'ぺ': ['p', 'e'], 'ぽ': ['p', 'o'],\n",
        "    'パ': ['p', 'a'], 'ピ': ['p', 'i'], 'プ': ['p', 'u'], 'ペ': ['p', 'e'], 'ポ': ['p', 'o'],\n",
        "\n",
        "    # Yoon (Contracted sounds) - Fixed: all keys are now kana\n",
        "    'きゃ': ['ky', 'a'], 'きゅ': ['ky', 'u'], 'きぇ': ['ky', 'e'], 'きょ': ['ky', 'o'],\n",
        "    'キャ': ['ky', 'a'], 'キュ': ['ky', 'u'], 'キェ': ['ky', 'e'], 'キョ': ['ky', 'o'],\n",
        "\n",
        "    'しゃ': ['sh', 'a'], 'しゅ': ['sh', 'u'], 'しぇ': ['sh', 'e'], 'しょ': ['sh', 'o'],\n",
        "    'シャ': ['sh', 'a'], 'シュ': ['sh', 'u'], 'シェ': ['sh', 'e'], 'ショ': ['sh', 'o'],\n",
        "\n",
        "    'ちゃ': ['ch', 'a'], 'ちゅ': ['ch', 'u'], 'ちぇ': ['ch', 'e'], 'ちょ': ['ch', 'o'],\n",
        "    'チャ': ['ch', 'a'], 'チュ': ['ch', 'u'], 'チェ': ['ch', 'e'], 'チョ': ['ch', 'o'],\n",
        "\n",
        "    'にゃ': ['ny', 'a'], 'にゅ': ['ny', 'u'], 'にぇ': ['ny', 'e'], 'にょ': ['ny', 'o'],\n",
        "    'ニャ': ['ny', 'a'], 'ニュ': ['ny', 'u'], 'ニェ': ['ny', 'e'], 'ニョ': ['ny', 'o'],\n",
        "\n",
        "    'ひゃ': ['hy', 'a'], 'ひゅ': ['hy', 'u'], 'ひぇ': ['hy', 'e'], 'ひょ': ['hy', 'o'],\n",
        "    'ヒャ': ['hy', 'a'], 'ヒュ': ['hy', 'u'], 'ヒェ': ['hy', 'e'], 'ヒョ': ['hy', 'o'],\n",
        "\n",
        "    'みゃ': ['my', 'a'], 'みゅ': ['my', 'u'], 'みぇ': ['my', 'e'], 'みょ': ['my', 'o'],\n",
        "    'ミャ': ['my', 'a'], 'ミュ': ['my', 'u'], 'ミェ': ['my', 'e'], 'ミョ': ['my', 'o'],\n",
        "\n",
        "    'りゃ': ['ry', 'a'], 'りゅ': ['ry', 'u'], 'りぇ': ['ry', 'e'], 'りょ': ['ry', 'o'],\n",
        "    'リャ': ['ry', 'a'], 'リュ': ['ry', 'u'], 'リェ': ['ry', 'e'], 'リョ': ['ry', 'o'],\n",
        "\n",
        "    'ぎゃ': ['gy', 'a'], 'ぎゅ': ['gy', 'u'], 'ぎぇ': ['gy', 'e'], 'ぎょ': ['gy', 'o'],\n",
        "    'ギャ': ['gy', 'a'], 'ギュ': ['gy', 'u'], 'ギェ': ['gy', 'e'], 'ギョ': ['gy', 'o'],\n",
        "\n",
        "    'じゃ': ['j', 'a'], 'じゅ': ['j', 'u'], 'じぇ': ['j', 'e'], 'じょ': ['j', 'o'],\n",
        "    'ジャ': ['j', 'a'], 'ジュ': ['j', 'u'], 'ジェ': ['j', 'e'], 'ジョ': ['j', 'o'],\n",
        "\n",
        "    'ぢゃ': ['j', 'a'], 'ぢゅ': ['j', 'u'], 'ぢぇ': ['j', 'e'], 'ぢょ': ['j', 'o'],\n",
        "    'ヂャ': ['j', 'a'], 'ヂュ': ['j', 'u'], 'ヂェ': ['j', 'e'], 'ヂョ': ['j', 'o'],\n",
        "\n",
        "    'でゃ': ['dy', 'a'], 'でゅ': ['dy', 'u'], 'でぇ': ['dy', 'e'], 'でょ': ['dy', 'o'],\n",
        "    'デャ': ['dy', 'a'], 'デュ': ['dy', 'u'], 'デェ': ['dy', 'e'], 'デョ': ['dy', 'o'],\n",
        "\n",
        "    'びゃ': ['by', 'a'], 'びゅ': ['by', 'u'], 'びぇ': ['by', 'e'], 'びょ': ['by', 'o'],\n",
        "    'ビャ': ['by', 'a'], 'ビュ': ['by', 'u'], 'ビェ': ['by', 'e'], 'ビョ': ['by', 'o'],\n",
        "\n",
        "    'ぴゃ': ['py', 'a'], 'ぴゅ': ['py', 'u'], 'ぴぇ': ['py', 'e'], 'ぴょ': ['py', 'o'],\n",
        "    'ピャ': ['py', 'a'], 'ピュ': ['py', 'u'], 'ピェ': ['py', 'e'], 'ピョ': ['py', 'o'],\n",
        "\n",
        "    # Extra / Special\n",
        "    'ふぁ': ['f', 'a'], 'ふぃ': ['f', 'i'], 'ふぇ': ['f', 'e'], 'ふぉ': ['f', 'o'],\n",
        "    'ファ': ['f', 'a'], 'フィ': ['f', 'i'], 'フェ': ['f', 'e'], 'フォ': ['f', 'o'],\n",
        "\n",
        "    'うぃ': ['w', 'i'], 'うぇ': ['w', 'e'], 'うぉ': ['w', 'o'],\n",
        "    'ウィ': ['w', 'i'], 'ウェ': ['w', 'e'], 'ウォ': ['w', 'o'],\n",
        "\n",
        "    'ヴ': ['b', 'u'], 'ゔ': ['b', 'u'],\n",
        "    'ヴァ': ['b', 'a'], 'ヴィ': ['b', 'i'], 'ヴェ': ['b', 'e'], 'ヴォ': ['b', 'o'],\n",
        "\n",
        "    'ゐ': ['w', 'i'], 'ヰ': ['w', 'i'],\n",
        "    'ゑ': ['w', 'e'], 'ヱ': ['w', 'e'],\n",
        "    'ヶ': ['k', 'e'], 'ヵ': ['k', 'a'],\n",
        "    'ゎ': ['w', 'a'], 'ヮ': ['w', 'a'],\n",
        "\n",
        "    'てぃ': ['t', 'i'], 'てゅ': ['t', 'u'], 'でぃ': ['d', 'i'], 'でゅ': ['d', 'u'],\n",
        "    'ティ': ['t', 'i'], 'テュ': ['t', 'u'], 'ディ': ['d', 'i'], 'デュ': ['d', 'u'],\n",
        "\n",
        "    'とぅ': ['t', 'u'], 'どぅ': ['d', 'u'],\n",
        "    'トゥ': ['t', 'u'], 'ドゥ': ['d', 'u'],\n",
        "\n",
        "    # Sokuon / Pause\n",
        "    'っ': ['cl'], 'ッ': ['cl'],\n",
        "    ' ': ['pau'], '　': ['pau'], '、': ['pau'], '。': ['pau'],\n",
        "}\n",
        "\n",
        "# Sorted keys by length (descending) to match compound sounds first\n",
        "SORTED_KEYS = sorted(KANA_MAPPING.keys(), key=len, reverse=True)\n",
        "\n",
        "def g2p_manual(text):\n",
        "    if not text: return ['SP']\n",
        "\n",
        "    # Normalize\n",
        "    text = text.replace('pau', ' ').replace('SP', ' ')\n",
        "\n",
        "    phonemes = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        matched = False\n",
        "        for k in SORTED_KEYS:\n",
        "            if text[i:].startswith(k):\n",
        "                phonemes.extend(KANA_MAPPING[k])\n",
        "                i += len(k)\n",
        "                matched = True\n",
        "                break\n",
        "\n",
        "        if not matched:\n",
        "            char = text[i]\n",
        "            if 'a' <= char <= 'z' or 'A' <= char <= 'Z':\n",
        "                pass\n",
        "            elif char in [' ', '　']:\n",
        "                phonemes.append('SP')\n",
        "            i += 1\n",
        "\n",
        "    if not phonemes: return ['SP']\n",
        "    return phonemes\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 4. ut_one.py — 完全内包 (ファイルそのまま、kana_map 依存を上記定義で解決済み)\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "ut_one.py ─ 歌声合成コアライブラリ（v9.0 プロ品質版）\n",
        "SynthesizerV / NEUTRINO / sinsy レベルの品質を目指す完全リライト。\n",
        "・Variance Adaptor（ピッチ/エネルギー残差予測）\n",
        "・Dilated Residual PostNet（広受容野スペクトル補正）\n",
        "・Multi-Resolution STFT Loss / LogMel Loss\n",
        "・手動DSP最小化（モデル出力をそのままWORLDに渡す設計）\n",
        "・WORLD ボコーダー使用（ボコーダー学習不要）\n",
        "\"\"\"\n",
        "\n",
        "# ==========================================\n",
        "# グローバル設定\n",
        "# ==========================================\n",
        "SAMPLE_RATE     = 44100\n",
        "FRAME_PERIOD_MS = 5.0\n",
        "MCEP_DIMENSION  = 128\n",
        "\n",
        "try:\n",
        "    AP_DIMENSION = pw.get_num_aperiodicities(SAMPLE_RATE)\n",
        "except AttributeError:\n",
        "    AP_DIMENSION = 5\n",
        "\n",
        "# ==========================================\n",
        "# 日本語音素辞書\n",
        "# ==========================================\n",
        "def create_phoneme_map():\n",
        "    phonemes_list = [\n",
        "        'SP', 'AP', 'N', 'a', 'b', 'by', 'ch', 'cl', 'd', 'dy', 'e', 'f',\n",
        "        'g', 'gy', 'h', 'hy', 'i', 'j', 'k', 'ky', 'm', 'my', 'n', 'ny',\n",
        "        'o', 'p', 'py', 'r', 'ry', 's', 'sh', 't', 'ts', 'u', 'w', 'y',\n",
        "        'z', 'br', 'pau'\n",
        "    ]\n",
        "    phonemes = set(phonemes_list)\n",
        "    sorted_p = sorted(list(phonemes - {'SP', 'AP', 'pau'}))\n",
        "    phoneme_map = {'PAD': 0, 'SP': 1, 'AP': 2}\n",
        "    for i, p in enumerate(sorted_p):\n",
        "        phoneme_map[p] = i + 3\n",
        "    return phoneme_map, len(phoneme_map)\n",
        "\n",
        "PHONEME_MAP, NUM_PHONEMES = create_phoneme_map()\n",
        "CONSONANTS = {\n",
        "    'b','by','ch','cl','d','dy','f','g','gy','h','hy','j',\n",
        "    'k','ky','m','my','n','ny','p','py','r','ry','s','sh',\n",
        "    't','ts','w','y','z','br','N'\n",
        "}\n",
        "VOWELS = {'a', 'i', 'u', 'e', 'o'}\n",
        "\n",
        "def is_consonant(p): return p in CONSONANTS\n",
        "def is_vowel(p):     return p in VOWELS\n",
        "\n",
        "# ==========================================\n",
        "# AIモデル定義（v9.0 プロ品質版）\n",
        "# ==========================================\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x): return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class RelativePositionEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe  = torch.zeros(max_len, dim)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, dim, 2).float() * (-np.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    \"\"\"ConformerBlock with FiLM speaker conditioning.\"\"\"\n",
        "    def __init__(self, dim, heads=8, kernel_size=31, dropout=0.1, ff_mult=4):\n",
        "        super().__init__()\n",
        "        ff_dim = dim * ff_mult\n",
        "        self.ff1 = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, ff_dim), Swish(),\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, dim), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.attn      = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
        "        self.norm_attn = nn.LayerNorm(dim)\n",
        "        self.norm_conv = nn.LayerNorm(dim)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(dim, dim * 2, 1), nn.GLU(dim=1),\n",
        "            nn.Conv1d(dim, dim, kernel_size, padding=kernel_size // 2, groups=dim),\n",
        "            nn.BatchNorm1d(dim), Swish(),\n",
        "            nn.Conv1d(dim, dim, 1), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.film_gamma = nn.Linear(dim, dim)\n",
        "        self.film_beta  = nn.Linear(dim, dim)\n",
        "        self.ff2 = nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, ff_dim), Swish(),\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, dim), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm_end = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, spk_emb=None):\n",
        "        x = x + 0.5 * self.ff1(x)\n",
        "        if spk_emb is not None:\n",
        "            g = self.film_gamma(spk_emb).unsqueeze(1)\n",
        "            b = self.film_beta(spk_emb).unsqueeze(1)\n",
        "            x = x * (1 + g) + b\n",
        "        res = x\n",
        "        xn  = self.norm_attn(x)\n",
        "        x, _ = self.attn(xn, xn, xn)\n",
        "        x   = res + x\n",
        "        res = x\n",
        "        xc  = self.norm_conv(x).transpose(1, 2)\n",
        "        xc  = self.conv(xc).transpose(1, 2)\n",
        "        x   = res + xc\n",
        "        x   = x + 0.5 * self.ff2(x)\n",
        "        return self.norm_end(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, dilation=1, dropout=0.05):\n",
        "        super().__init__()\n",
        "        pad        = dilation * (kernel_size - 1) // 2\n",
        "        self.conv1 = weight_norm(nn.Conv1d(channels, channels, kernel_size,\n",
        "                                           padding=pad, dilation=dilation))\n",
        "        self.bn1   = nn.BatchNorm1d(channels)\n",
        "        self.conv2 = weight_norm(nn.Conv1d(channels, channels, kernel_size,\n",
        "                                           padding=kernel_size // 2))\n",
        "        self.bn2   = nn.BatchNorm1d(channels)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x   = F.leaky_relu(self.bn1(self.conv1(x)), 0.1)\n",
        "        x   = self.drop(x)\n",
        "        x   = self.bn2(self.conv2(x))\n",
        "        return F.leaky_relu(x + res, 0.1)\n",
        "\n",
        "\n",
        "class DilatedPostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Dilated Residual PostNet — 広い受容野でスペクトル包絡を精緻に補正。\n",
        "    5層のDilated Conv (dilation 1,2,4,8,1) で受容野を大幅拡大。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden_dim=512, kernel_size=5, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pre = nn.Sequential(\n",
        "            weight_norm(nn.Conv1d(in_dim, hidden_dim, 1)),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        dilations = [1, 2, 4, 8, 1]\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for d in dilations:\n",
        "            pad = d * (kernel_size - 1) // 2\n",
        "            self.blocks.append(nn.Sequential(\n",
        "                nn.LeakyReLU(0.1),\n",
        "                weight_norm(nn.Conv1d(hidden_dim, hidden_dim, kernel_size,\n",
        "                                      dilation=d, padding=pad)),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Dropout(dropout),\n",
        "                weight_norm(nn.Conv1d(hidden_dim, hidden_dim, 1)),\n",
        "            ))\n",
        "        self.post = weight_norm(nn.Conv1d(hidden_dim, in_dim, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        res_orig = x\n",
        "        x = x.transpose(1, 2)  # -> (B, C, T)\n",
        "        x = self.pre(x)\n",
        "        for blk in self.blocks:\n",
        "            x = x + blk(x)     # residual\n",
        "        x = self.post(x)\n",
        "        x = x.transpose(1, 2)  # -> (B, T, C)\n",
        "        return res_orig + x\n",
        "\n",
        "\n",
        "class VariancePredictor(nn.Module):\n",
        "    \"\"\"軽量 Variance Predictor (2-layer Conv1D)\"\"\"\n",
        "    def __init__(self, hidden_dim, out_dim=1, kernel_size=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.proj = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        out = self.net(x.transpose(1, 2)).transpose(1, 2)\n",
        "        return self.proj(out)\n",
        "\n",
        "\n",
        "class SingingModel(nn.Module):\n",
        "    \"\"\"\n",
        "    v9.0 Generator: ResNet → Conformer×2 → BiLSTM → Conformer×2 →\n",
        "                     VarianceAdaptor(energy) → Heads + DilatedPostNet\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_phonemes: int  = NUM_PHONEMES,\n",
        "        mcep_dim: int      = MCEP_DIMENSION,\n",
        "        ap_dim: int        = AP_DIMENSION,\n",
        "        embedding_dim: int = 384,\n",
        "        hidden_dim: int    = 512,\n",
        "        num_speakers: int  = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.phoneme_embedding = nn.Embedding(num_phonemes, embedding_dim, padding_idx=0)\n",
        "        self.speaker_embedding = nn.Embedding(num_speakers, hidden_dim)\n",
        "        self.pitch_embedding   = nn.Embedding(128, hidden_dim)\n",
        "        self.pos_enc           = RelativePositionEncoding(hidden_dim)\n",
        "\n",
        "        # 入力: prev/curr/next phoneme emb + 3 numeric features + 1 energy\n",
        "        input_dim = embedding_dim * 3 + 3\n",
        "        self.pre_conv = nn.Sequential(\n",
        "            weight_norm(nn.Conv1d(input_dim, hidden_dim, 3, padding=1)),\n",
        "            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResBlock(hidden_dim, 3, 1), ResBlock(hidden_dim, 3, 2),\n",
        "            ResBlock(hidden_dim, 3, 4), ResBlock(hidden_dim, 3, 8),\n",
        "        ])\n",
        "        self.conf_pre = nn.ModuleList([\n",
        "            ConformerBlock(hidden_dim, heads=8, kernel_size=15, ff_mult=4),\n",
        "            ConformerBlock(hidden_dim, heads=8, kernel_size=31, ff_mult=4),\n",
        "        ])\n",
        "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim // 2,\n",
        "                            num_layers=3, batch_first=True,\n",
        "                            bidirectional=True, dropout=0.1)\n",
        "        self.conf_post = nn.ModuleList([\n",
        "            ConformerBlock(hidden_dim, heads=8, kernel_size=15, ff_mult=4),\n",
        "            ConformerBlock(hidden_dim, heads=8, kernel_size=31, ff_mult=4),\n",
        "        ])\n",
        "\n",
        "        # Variance Predictor (energy residual)\n",
        "        self.energy_predictor = VariancePredictor(hidden_dim, out_dim=1)\n",
        "\n",
        "        # Output heads (3-layer MLP for better expressiveness)\n",
        "        self.mgc_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), Swish(), nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2), Swish(),\n",
        "            nn.Linear(hidden_dim // 2, mcep_dim)\n",
        "        )\n",
        "        self.lf0_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2), Swish(),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4), Swish(),\n",
        "            nn.Linear(hidden_dim // 4, 1)\n",
        "        )\n",
        "        self.vuv_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 4), Swish(),\n",
        "            nn.Linear(hidden_dim // 4, 1)\n",
        "        )\n",
        "        self.bap_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 4), Swish(),\n",
        "            nn.Linear(hidden_dim // 4, ap_dim)\n",
        "        )\n",
        "\n",
        "        # Dilated Residual PostNets\n",
        "        self.mgc_postnet = DilatedPostNet(mcep_dim, hidden_dim=256, kernel_size=5, dropout=0.1)\n",
        "        self.bap_postnet = DilatedPostNet(ap_dim,   hidden_dim=128, kernel_size=3, dropout=0.1)\n",
        "\n",
        "    def forward(self, ph_ids, feats, spk_id=None, pitch_id=None):\n",
        "        prev_e = self.phoneme_embedding(ph_ids[:, :, 0])\n",
        "        curr_e = self.phoneme_embedding(ph_ids[:, :, 1])\n",
        "        next_e = self.phoneme_embedding(ph_ids[:, :, 2])\n",
        "        x      = torch.cat([prev_e, curr_e, next_e, feats], dim=-1)\n",
        "\n",
        "        bs = x.shape[0]\n",
        "        if spk_id is None:\n",
        "            spk_id = torch.zeros(bs, device=x.device, dtype=torch.long)\n",
        "        spk_emb = self.speaker_embedding(spk_id)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        for blk in self.res_blocks:\n",
        "            x = blk(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        if pitch_id is not None:\n",
        "            x = x + self.pitch_embedding(pitch_id.clamp(0, 127))\n",
        "\n",
        "        x = self.pos_enc(x)\n",
        "        for blk in self.conf_pre:\n",
        "            x = blk(x, spk_emb)\n",
        "        x, _ = self.lstm(x)\n",
        "        for blk in self.conf_post:\n",
        "            x = blk(x, spk_emb)\n",
        "\n",
        "        # Variance prediction\n",
        "        energy_pred = self.energy_predictor(x)\n",
        "\n",
        "        # Output heads\n",
        "        mgc_pre  = self.mgc_head(x)\n",
        "        lf0      = self.lf0_head(x)\n",
        "        vuv      = self.vuv_head(x)\n",
        "        bap_pre  = self.bap_head(x)\n",
        "\n",
        "        # PostNet refinement\n",
        "        mgc_post = self.mgc_postnet(mgc_pre)\n",
        "        bap_post = self.bap_postnet(bap_pre)\n",
        "\n",
        "        return mgc_pre, mgc_post, lf0, vuv, bap_pre, bap_post, energy_pred\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# モデルロードユーティリティ\n",
        "# ==========================================\n",
        "\n",
        "def load_model_weights_safe(model, state_dict, log_func=None):\n",
        "    def _log(m):\n",
        "        if log_func: log_func(m)\n",
        "        else: print(m)\n",
        "\n",
        "    new_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith(\"conformer.0.\"):\n",
        "            new_dict[k.replace(\"conformer.0.\", \"conf_pre.0.\", 1)] = v\n",
        "        elif k.startswith(\"conformer.1.\"):\n",
        "            new_dict[k.replace(\"conformer.1.\", \"conf_pre.1.\", 1)] = v\n",
        "        elif k.startswith(\"conf1.\"):\n",
        "            new_dict[k.replace(\"conf1.\", \"conf_pre.0.\", 1)] = v\n",
        "        elif k.startswith(\"conf2.\"):\n",
        "            new_dict[k.replace(\"conf2.\", \"conf_pre.1.\", 1)] = v\n",
        "        else:\n",
        "            new_dict[k] = v\n",
        "\n",
        "    final_dict = {}\n",
        "    for k, v in new_dict.items():\n",
        "        if \"parametrizations.\" in k and \".original\" in k:\n",
        "            parts = k.split('.')\n",
        "            try:\n",
        "                idx     = parts.index('parametrizations')\n",
        "                base    = \".\".join(parts[:idx])\n",
        "                pname   = parts[idx + 1]\n",
        "                suffix  = parts[-1]\n",
        "                new_key = f\"{base}.{pname}_g\" if suffix.endswith('0') else f\"{base}.{pname}_v\"\n",
        "                final_dict[new_key] = v\n",
        "            except (ValueError, IndexError):\n",
        "                final_dict[k] = v\n",
        "        else:\n",
        "            final_dict[k] = v\n",
        "\n",
        "    model_state = model.state_dict()\n",
        "    missing = [k for k in model_state if k not in final_dict]\n",
        "    if missing:\n",
        "        _log(f\"⚠️ Missing {len(missing)} keys (新規初期化)\")\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(final_dict, strict=False)\n",
        "        _log(\"✅ モデルロード成功\")\n",
        "    except RuntimeError as e:\n",
        "        _log(f\"❌ ロード失敗: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# GV ユーティリティ\n",
        "# ==========================================\n",
        "\n",
        "def calculate_gv(data):\n",
        "    return np.mean(data, axis=0), np.var(data, axis=0)\n",
        "\n",
        "def apply_gv(gen, gv_mean, gv_var, alpha=1.0):\n",
        "    gm    = np.mean(gen, axis=0)\n",
        "    gs    = np.std(gen, axis=0)\n",
        "    ts    = np.sqrt(np.maximum(gv_var, 1e-8))\n",
        "    gs    = np.maximum(gs, 1e-8)\n",
        "    ratio = ts / gs\n",
        "    return gm + (gen - gm) * (ratio ** alpha)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 音声処理関数群（v9.0 最小DSP版）\n",
        "# ==========================================\n",
        "\n",
        "def get_fft_size(sr):\n",
        "    if sr >= 44100: return 4096\n",
        "    elif sr >= 24000: return 2048\n",
        "    return 1024\n",
        "\n",
        "\n",
        "def apply_high_pass_filter(y, sr, cutoff=60.0):\n",
        "    \"\"\"HPF — 低域ランブル除去\"\"\"\n",
        "    sos = signal.butter(4, cutoff, 'hp', fs=sr, output='sos')\n",
        "    return signal.sosfilt(sos, y)\n",
        "\n",
        "\n",
        "def multiband_compress(y, sr):\n",
        "    \"\"\"マルチバンドコンプレッション（軽量版）\"\"\"\n",
        "    sos_lo  = signal.butter(4, 200,          'lp', fs=sr, output='sos')\n",
        "    sos_mid = signal.butter(4, [200, 2000],  'bp', fs=sr, output='sos')\n",
        "    sos_hi  = signal.butter(4, [2000, 8000], 'bp', fs=sr, output='sos')\n",
        "    sos_air = signal.butter(4, 8000,         'hp', fs=sr, output='sos')\n",
        "    lo  = signal.sosfilt(sos_lo,  y)\n",
        "    mid = signal.sosfilt(sos_mid, y)\n",
        "    hi  = signal.sosfilt(sos_hi,  y)\n",
        "    air = signal.sosfilt(sos_air, y)\n",
        "\n",
        "    def rms_compress(band, thr=0.3, ratio=2.5, makeup=1.0):\n",
        "        rms  = np.sqrt(np.mean(band ** 2) + 1e-12)\n",
        "        gain = thr / rms * (rms / thr) ** (1.0 / ratio) if rms > thr else 1.0\n",
        "        return band * gain * makeup\n",
        "\n",
        "    lo  = rms_compress(lo,  thr=0.55, ratio=3.0, makeup=0.85)\n",
        "    mid = rms_compress(mid, thr=0.4,  ratio=3.0, makeup=1.0)\n",
        "    hi  = rms_compress(hi,  thr=0.35, ratio=3.5, makeup=1.0)\n",
        "    air = rms_compress(air, thr=0.2,  ratio=2.0, makeup=0.95)\n",
        "    return lo + mid + hi + air\n",
        "\n",
        "\n",
        "def soft_limiter(y, threshold=0.92):\n",
        "    \"\"\"ソフトリミッター\"\"\"\n",
        "    y    = y - np.mean(y)\n",
        "    peak = np.max(np.abs(y))\n",
        "    if peak < threshold:\n",
        "        return y\n",
        "    y = np.tanh(y / threshold * 1.05) * threshold * 0.97\n",
        "    return y\n",
        "\n",
        "\n",
        "def normalize_f0(f0_hz):\n",
        "    lf0 = np.zeros_like(f0_hz, dtype=np.float32)\n",
        "    v   = f0_hz > 0\n",
        "    if v.any(): lf0[v] = np.log(f0_hz[v])\n",
        "    return lf0\n",
        "\n",
        "\n",
        "def synthesis_wav(f0, mcep, ap, sr, high_quality=True,\n",
        "                  vuv=None, formant_strength=1.0,\n",
        "                  breath_intensity=0.0015,\n",
        "                  **kwargs) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    v9.0 高品質歌声合成パイプライン\n",
        "    設計思想: モデルが品質を担保するため、手動DSPを最小限に。\n",
        "    WORLDに直接渡して合成する。\n",
        "    \"\"\"\n",
        "    fft_size = get_fft_size(sr)\n",
        "    f0 = f0.astype(np.float64)\n",
        "    f0 = np.nan_to_num(f0, nan=0.0, posinf=2000.0, neginf=0.0)\n",
        "    f0 = np.clip(f0, 0.0, 2000.0)\n",
        "\n",
        "    # スペクトル包絡デコード\n",
        "    sp = pw.decode_spectral_envelope(mcep.astype(np.float64), sr, fft_size)\n",
        "    sp = np.maximum(sp, 1e-14)\n",
        "    sp = np.nan_to_num(sp, nan=1e-14, posinf=1e-14, neginf=1e-14)\n",
        "\n",
        "    # 異常値クリッピング\n",
        "    sp_max = np.percentile(sp, 99.9)\n",
        "    sp = np.clip(sp, 1e-14, max(sp_max, 1e-6))\n",
        "\n",
        "    # 非周期成分デコード\n",
        "    ap_full = pw.decode_aperiodicity(ap.astype(np.float64), sr, fft_size)\n",
        "    ap_full = np.clip(ap_full, 0.0, 0.999)\n",
        "    ap_full = np.nan_to_num(ap_full, nan=0.0)\n",
        "\n",
        "    # WORLD合成\n",
        "    f0      = np.ascontiguousarray(f0)\n",
        "    sp      = np.ascontiguousarray(sp)\n",
        "    ap_full = np.ascontiguousarray(ap_full)\n",
        "\n",
        "    y = pw.synthesize(f0, sp, ap_full, sr, frame_period=FRAME_PERIOD_MS)\n",
        "\n",
        "    if high_quality:\n",
        "        # 息継ぎ音（自然さのため）\n",
        "        if breath_intensity > 0 and vuv is not None:\n",
        "            unvoiced = np.zeros(len(f0), dtype=bool)\n",
        "            uv = vuv.flatten() if hasattr(vuv, 'flatten') else np.array(vuv)\n",
        "            unvoiced[:len(uv)] = uv[:len(unvoiced)] < 0.5\n",
        "            if np.any(unvoiced):\n",
        "                breath    = np.random.randn(len(y)) * breath_intensity\n",
        "                sos       = signal.butter(6, [100, 8000], 'bp', fs=sr, output='sos')\n",
        "                breath    = signal.sosfilt(sos, breath)\n",
        "                uv_interp = np.interp(\n",
        "                    np.arange(len(y)),\n",
        "                    np.linspace(0, len(y) - 1, len(f0)),\n",
        "                    unvoiced.astype(float)\n",
        "                )\n",
        "                uv_interp = gaussian_filter1d(uv_interp, 5.0)\n",
        "                y = y + breath * uv_interp\n",
        "\n",
        "        # HPF\n",
        "        y = apply_high_pass_filter(y, sr, cutoff=60.0)\n",
        "\n",
        "    # 正規化\n",
        "    y    = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    y    = y - np.mean(y)\n",
        "    peak = np.max(np.abs(y))\n",
        "    if peak > 1e-6:\n",
        "        y = y / peak\n",
        "    y = np.clip(y, -1.0, 1.0)\n",
        "\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# G2P / UST パース\n",
        "# ==========================================\n",
        "\n",
        "def g2p_kana(kana):\n",
        "    \"\"\"かな → 音素列 (内包した g2p_manual を使用)\"\"\"\n",
        "    if not kana or kana.lower() == 'r': return ['SP']\n",
        "    try:\n",
        "        phs = g2p_manual(kana)  # ← 上で定義した kana_map の g2p_manual を直接呼ぶ\n",
        "        out = []\n",
        "        for p in phs:\n",
        "            if p == 'pau':         out.append('SP')\n",
        "            elif p == 'cl':        out.append('cl')\n",
        "            elif p in PHONEME_MAP: out.append(p)\n",
        "        return out if out else ['SP']\n",
        "    except:\n",
        "        return ['SP']\n",
        "\n",
        "\n",
        "def parse_ust(ust_file):\n",
        "    notes, tempo = [], 120.0\n",
        "    try:\n",
        "        with open(ust_file, 'r', encoding='cp932') as f:\n",
        "            lines = f.readlines()\n",
        "    except:\n",
        "        try:\n",
        "            with open(ust_file, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            return notes, tempo\n",
        "\n",
        "    curr, in_sec, current_tempo = {}, False, 120.0\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        if line.startswith('[#'):\n",
        "            if in_sec and 'Lyric' in curr:\n",
        "                curr.setdefault('Tempo', current_tempo)\n",
        "                notes.append(curr)\n",
        "            in_sec = True; curr = {}\n",
        "        elif line.startswith('Tempo='):\n",
        "            try: current_tempo = float(line.split('=')[1])\n",
        "            except: pass\n",
        "        elif in_sec and '=' in line:\n",
        "            k, v = line.split('=', 1)\n",
        "            curr[k] = v\n",
        "\n",
        "    if in_sec and 'Lyric' in curr:\n",
        "        curr.setdefault('Tempo', current_tempo)\n",
        "        notes.append(curr)\n",
        "\n",
        "    return notes, tempo\n",
        "\n",
        "\n",
        "def decode_ust_pitch(note, dur_frames, frame_period_ms):\n",
        "    \"\"\"USTのPitchBend + VBRデコード\"\"\"\n",
        "    curve  = np.zeros(dur_frames, dtype=np.float32)\n",
        "    pb_str = note.get('PitchBend', '')\n",
        "    if pb_str:\n",
        "        try:\n",
        "            pb = [float(x) for x in pb_str.split(',') if x]\n",
        "            if pb:\n",
        "                idxs   = np.linspace(0, len(pb) - 1, dur_frames)\n",
        "                curve += np.interp(idxs, np.arange(len(pb)), pb).astype(np.float32)\n",
        "        except: pass\n",
        "\n",
        "    vbr_str = note.get('VBR', '')\n",
        "    if vbr_str:\n",
        "        try:\n",
        "            vp = [float(x) for x in vbr_str.split(',')]\n",
        "            if len(vp) >= 3 and vp[0] > 0:\n",
        "                vlen   = vp[0] / 100.0\n",
        "                period = vp[1]\n",
        "                depth  = vp[2]\n",
        "                vin    = vp[3] / 100.0 if len(vp) > 3 else 0.1\n",
        "                vout   = vp[4] / 100.0 if len(vp) > 4 else 0.1\n",
        "                phase  = vp[5] if len(vp) > 5 else 0.0\n",
        "                offset = vp[6] if len(vp) > 6 else 0.0\n",
        "                vf     = int(dur_frames * vlen)\n",
        "                sf_    = dur_frames - vf\n",
        "                if sf_ < 0: sf_ = 0\n",
        "                cf     = dur_frames - sf_\n",
        "                if cf > 0:\n",
        "                    t   = np.arange(cf) * frame_period_ms\n",
        "                    vib = (np.sin(2 * np.pi * t / period +\n",
        "                                  phase / 100.0 * 2 * np.pi) * depth + offset)\n",
        "                    env = np.ones(cf)\n",
        "                    fin = int(cf * vin); fout = int(cf * vout)\n",
        "                    if fin  > 0: env[:fin]   = np.linspace(0, 1, fin)\n",
        "                    if fout > 0: env[-fout:] = np.linspace(1, 0, fout)\n",
        "                    curve[sf_:] += vib * env\n",
        "        except: pass\n",
        "\n",
        "    return curve\n",
        "\n",
        "\n",
        "def ust_to_features(notes, default_tempo):\n",
        "    \"\"\"UST → 特徴量変換\"\"\"\n",
        "    timed_ph, last_vowel = [], None\n",
        "    for note in notes:\n",
        "        try:\n",
        "            length = float(note.get('Length', 480))\n",
        "            lyric  = note.get('Lyric', 'r')\n",
        "            num    = int(note.get('NoteNum', 60))\n",
        "            tempo  = float(note.get('Tempo', default_tempo))\n",
        "        except: continue\n",
        "\n",
        "        ms_per_tick = 60000 / (tempo * 480)\n",
        "        dur_ms = length * ms_per_tick\n",
        "        f0_hz  = 440.0 * (2.0 ** ((num - 69) / 12.0))\n",
        "        lf0    = np.log(f0_hz) if lyric != 'r' else 0.0\n",
        "        pid    = num if lyric != 'r' else 0\n",
        "\n",
        "        if lyric.strip() in ('+', '-', '_'):\n",
        "            phs = [last_vowel] if last_vowel and lf0 != 0.0 else ['SP']\n",
        "        elif lyric.lower() == 'r':\n",
        "            phs = ['SP']\n",
        "        else:\n",
        "            phs = g2p_kana(lyric)\n",
        "\n",
        "        if not phs: continue\n",
        "        ph_dur = dur_ms / len(phs)\n",
        "        for p in phs:\n",
        "            timed_ph.append({'p': p, 'dur_ms': ph_dur, 'lf0': lf0,\n",
        "                              'pitch_id': pid, 'note_raw': note})\n",
        "        cv = next((p for p in reversed(phs) if p in VOWELS), None)\n",
        "        if cv and lyric.strip() not in ('+', '-', '_'):\n",
        "            last_vowel = cv\n",
        "\n",
        "    if not timed_ph:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    total_ms   = sum(t['dur_ms'] for t in timed_ph)\n",
        "    tf         = int(total_ms / FRAME_PERIOD_MS) + 100\n",
        "    ids        = np.zeros((tf, 3), dtype=np.int64)\n",
        "    nums       = np.zeros((tf, 3), dtype=np.float32)\n",
        "    p_ids      = np.zeros(tf, dtype=np.int64)\n",
        "    ph_id_list = [PHONEME_MAP.get(t['p'], PHONEME_MAP['SP']) for t in timed_ph]\n",
        "\n",
        "    cur_ms, last_f, cur_f = 0.0, 0, 0\n",
        "    for i, tp in enumerate(timed_ph):\n",
        "        end_ms = cur_ms + tp['dur_ms']\n",
        "        s = last_f\n",
        "        e = min(max(s + 1, int(end_ms / FRAME_PERIOD_MS)), tf)\n",
        "        if s >= tf: break\n",
        "\n",
        "        ids[s:e, 0] = ph_id_list[i-1] if i > 0 else PHONEME_MAP['SP']\n",
        "        ids[s:e, 1] = ph_id_list[i]\n",
        "        ids[s:e, 2] = ph_id_list[i+1] if i < len(ph_id_list)-1 else PHONEME_MAP['SP']\n",
        "\n",
        "        frames = e - s\n",
        "        if frames > 0:\n",
        "            nums[s:e, 0] = np.arange(frames) / frames\n",
        "            if 'note_raw' in tp and tp['note_raw']:\n",
        "                dev = decode_ust_pitch(tp['note_raw'], frames, FRAME_PERIOD_MS)\n",
        "                nums[s:e, 2] = tp['lf0'] + (dev / 1200.0) * np.log(2.0)\n",
        "            else:\n",
        "                nums[s:e, 2] = tp['lf0']\n",
        "        nums[s:e, 1] = frames\n",
        "        p_ids[s:e]   = tp['pitch_id']\n",
        "        cur_ms = end_ms; last_f = e; cur_f = e\n",
        "\n",
        "    return ids[:cur_f], nums[:cur_f], p_ids[:cur_f]\n",
        "\n",
        "\n",
        "def ust_to_features_fixed_consonant(notes, default_tempo, consonant_length_ms=60.0):\n",
        "    \"\"\"子音固定長モード\"\"\"\n",
        "    segs, last_vowel = [], None\n",
        "    for note in notes:\n",
        "        try:\n",
        "            length = float(note.get('Length', 480))\n",
        "            lyric  = note.get('Lyric', 'r')\n",
        "            num    = int(note.get('NoteNum', 60))\n",
        "            tempo  = float(note.get('Tempo', default_tempo))\n",
        "        except: continue\n",
        "\n",
        "        ms_per_tick = 60000 / (tempo * 480)\n",
        "        dur_ms = length * ms_per_tick\n",
        "        f0_hz  = 440.0 * (2.0 ** ((num - 69) / 12.0))\n",
        "        lf0    = np.log(f0_hz) if lyric != 'r' else 0.0\n",
        "        pid    = num if lyric != 'r' else 0\n",
        "        phs, durs, cv = [], [], None\n",
        "\n",
        "        if lyric.strip() in ('+', '-', '_'):\n",
        "            if last_vowel and lf0 != 0.0:\n",
        "                phs, durs, cv = [last_vowel], [dur_ms], last_vowel\n",
        "            else:\n",
        "                phs, durs = ['SP'], [dur_ms]\n",
        "        elif lyric.lower() == 'r':\n",
        "            phs, durs = ['SP'], [dur_ms]\n",
        "        else:\n",
        "            phs_raw = g2p_kana(lyric)\n",
        "            if not phs_raw:\n",
        "                phs, durs = ['SP'], [dur_ms]\n",
        "            else:\n",
        "                phs   = phs_raw\n",
        "                cons  = [p for p in phs if is_consonant(p)]\n",
        "                vows  = [p for p in phs if not is_consonant(p)]\n",
        "                c_len = min(consonant_length_ms,\n",
        "                            (dur_ms * 0.65) / max(len(cons), 1))\n",
        "                rem   = dur_ms - len(cons) * c_len\n",
        "                v_len = rem / max(len(vows), 1)\n",
        "                assigned = 0.0\n",
        "                for p in phs:\n",
        "                    d = c_len if is_consonant(p) else v_len\n",
        "                    durs.append(d); assigned += d\n",
        "                if durs: durs[-1] += (dur_ms - assigned)\n",
        "                cv = next((p for p in reversed(phs) if p in VOWELS), None)\n",
        "\n",
        "        if phs:\n",
        "            segs.append({'phs': phs, 'durs': durs, 'lf0': lf0,\n",
        "                         'pitch_id': pid, 'note_raw': note})\n",
        "            if cv and lyric.strip() not in ('+', '-', '_'):\n",
        "                last_vowel = cv\n",
        "\n",
        "    if not segs:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    total_ms = sum(sum(s['durs']) for s in segs)\n",
        "    tf       = int(total_ms / FRAME_PERIOD_MS) + 100\n",
        "    ids      = np.zeros((tf, 3), dtype=np.int64)\n",
        "    nums     = np.zeros((tf, 3), dtype=np.float32)\n",
        "    p_ids    = np.zeros(tf, dtype=np.int64)\n",
        "\n",
        "    all_p, all_d, all_l, all_pi, all_notes = [], [], [], [], []\n",
        "    for s in segs:\n",
        "        all_p.extend(s['phs']);  all_d.extend(s['durs'])\n",
        "        all_l.extend([s['lf0']] * len(s['phs']))\n",
        "        all_pi.extend([s['pitch_id']] * len(s['phs']))\n",
        "        for j in range(len(s['phs'])):\n",
        "            all_notes.append(s['note_raw'] if j == 0 else None)\n",
        "\n",
        "    ph_id_list            = [PHONEME_MAP.get(p, PHONEME_MAP['SP']) for p in all_p]\n",
        "    cur_ms, last_f, cur_f = 0.0, 0, 0\n",
        "\n",
        "    for i, (p, d_ms, lf0, pi, note_raw) in enumerate(\n",
        "            zip(all_p, all_d, all_l, all_pi, all_notes)):\n",
        "        end_ms = cur_ms + d_ms\n",
        "        s = last_f\n",
        "        e = min(max(s + 1, int(end_ms / FRAME_PERIOD_MS)), tf)\n",
        "        if s >= tf: break\n",
        "\n",
        "        ids[s:e, 0] = ph_id_list[i-1] if i > 0 else PHONEME_MAP['SP']\n",
        "        ids[s:e, 1] = ph_id_list[i]\n",
        "        ids[s:e, 2] = ph_id_list[i+1] if i < len(ph_id_list)-1 else PHONEME_MAP['SP']\n",
        "\n",
        "        frames = e - s\n",
        "        if frames > 0:\n",
        "            nums[s:e, 0] = np.arange(frames) / frames\n",
        "            if note_raw is not None:\n",
        "                dev = decode_ust_pitch(note_raw, frames, FRAME_PERIOD_MS)\n",
        "                nums[s:e, 2] = lf0 + (dev / 1200.0) * np.log(2.0)\n",
        "            else:\n",
        "                nums[s:e, 2] = lf0\n",
        "        nums[s:e, 1] = frames\n",
        "        p_ids[s:e]   = pi\n",
        "        cur_ms = end_ms; last_f = e; cur_f = e\n",
        "\n",
        "    return ids[:cur_f], nums[:cur_f], p_ids[:cur_f]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5. モデルキャッシュ / チャンク推論\n",
        "# =============================================================================\n",
        "_cached_path   = None\n",
        "_cached_model  = None\n",
        "_cached_scaler = None\n",
        "_device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🔧 デバイス: {_device}\")\n",
        "\n",
        "\n",
        "def _load_model(model_path: str):\n",
        "    global _cached_path, _cached_model, _cached_scaler\n",
        "\n",
        "    model_path = model_path.strip()\n",
        "    if _cached_path == model_path and _cached_model is not None:\n",
        "        return _cached_model, _cached_scaler, \"（キャッシュから読み込み）\"\n",
        "\n",
        "    logs = []\n",
        "\n",
        "    if model_path.lower().endswith(\".yvs\"):\n",
        "        logs.append(\"ℹ️ 統合モデルファイル (.yvs) を読み込んでいます...\")\n",
        "        try:\n",
        "            ckpt = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
        "        except TypeError:\n",
        "            ckpt = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "        if not (isinstance(ckpt, dict) and \"model_state_dict\" in ckpt):\n",
        "            raise ValueError(\"未対応または破損した .yvs ファイルです。\")\n",
        "\n",
        "        sc         = ckpt[\"scaler\"]\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "        config     = ckpt.get(\"config\", {})\n",
        "        hidden_dim = config.get(\"hidden_dim\", 512)\n",
        "        mtype      = config.get(\"model_type\", \"unknown\")\n",
        "\n",
        "        if mtype == \"base\":\n",
        "            logs.append(\"✅ V1 汎用モデルを検出\")\n",
        "        elif mtype == \"adapted\":\n",
        "            logs.append(f\"✅ V2 話者適応モデルを検出 (ベース: {config.get('base_model','?')})\")\n",
        "        else:\n",
        "            logs.append(\"✅ モデル読み込み完了\")\n",
        "    else:\n",
        "        logs.append(\"⚠️ 従来の .pth ファイルを読み込みます。\")\n",
        "        s_path = model_path.replace(\".pth\", \"_scaler.pkl\")\n",
        "        if not os.path.exists(s_path):\n",
        "            s_path = model_path.replace(\".pth\", \".pyl\")\n",
        "        with open(s_path, \"rb\") as f:\n",
        "            sc = pickle.load(f)\n",
        "        try:\n",
        "            state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
        "        except TypeError:\n",
        "            state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "        hidden_dim = 512\n",
        "\n",
        "    logs.append(\"🤖 v9.0 モデルをロード中...\")\n",
        "    model = SingingModel(hidden_dim=hidden_dim)\n",
        "    load_model_weights_safe(model, state_dict, log_func=lambda m: logs.append(m))\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        model = model.to(_device)\n",
        "    except RuntimeError as re:\n",
        "        if \"out of memory\" in str(re):\n",
        "            logs.append(\"⚠️ GPU メモリ不足のため CPU モードに切り替えます\")\n",
        "            model = model.to(torch.device(\"cpu\"))\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    _cached_path   = model_path\n",
        "    _cached_model  = model\n",
        "    _cached_scaler = sc\n",
        "    return model, sc, \"\\n\".join(logs)\n",
        "\n",
        "\n",
        "def _inference_chunked(model, ids, nums, p_ids, chunk_size=1500, overlap=300):\n",
        "    \"\"\"チャンク推論 (v9.0: 7出力対応・ハニング窓ブレンド)\"\"\"\n",
        "    total = ids.shape[0]\n",
        "\n",
        "    def _forward(id_t, nu_t, pi_t):\n",
        "        mgc_pre, mgc_post, lf0, vuv, bap_pre, bap_post, energy = model(id_t, nu_t, pitch_id=pi_t)\n",
        "        return (\n",
        "            mgc_post.squeeze(0).cpu().numpy(),\n",
        "            lf0.squeeze(0).cpu().numpy(),\n",
        "            torch.sigmoid(vuv.squeeze(0)).cpu().numpy(),\n",
        "            bap_post.squeeze(0).cpu().numpy(),\n",
        "        )\n",
        "\n",
        "    if total <= chunk_size:\n",
        "        with torch.no_grad():\n",
        "            return _forward(\n",
        "                torch.LongTensor(ids).unsqueeze(0).to(_device),\n",
        "                torch.FloatTensor(nums).unsqueeze(0).to(_device),\n",
        "                torch.LongTensor(p_ids).unsqueeze(0).to(_device),\n",
        "            )\n",
        "\n",
        "    step    = chunk_size - overlap\n",
        "    mgc_acc = np.zeros((total, MCEP_DIMENSION), dtype=np.float32)\n",
        "    lf0_acc = np.zeros((total, 1),              dtype=np.float32)\n",
        "    vuv_acc = np.zeros((total, 1),              dtype=np.float32)\n",
        "    bap_acc = np.zeros((total, AP_DIMENSION),   dtype=np.float32)\n",
        "    weight  = np.zeros(total,                    dtype=np.float32)\n",
        "\n",
        "    hann     = np.hanning(overlap * 2).astype(np.float32)\n",
        "    fade_in  = hann[:overlap]\n",
        "    fade_out = hann[overlap:]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start = 0\n",
        "        while start < total:\n",
        "            end = min(start + chunk_size, total)\n",
        "            m, l, v, b = _forward(\n",
        "                torch.LongTensor(ids[start:end]).unsqueeze(0).to(_device),\n",
        "                torch.FloatTensor(nums[start:end]).unsqueeze(0).to(_device),\n",
        "                torch.LongTensor(p_ids[start:end]).unsqueeze(0).to(_device),\n",
        "            )\n",
        "            L = end - start\n",
        "            w = np.ones(L, dtype=np.float32)\n",
        "            if start > 0:\n",
        "                fi = min(overlap, L); w[:fi]  *= fade_in[:fi]\n",
        "            if end < total:\n",
        "                fo = min(overlap, L); w[-fo:] *= fade_out[-fo:]\n",
        "\n",
        "            mgc_acc[start:end] += m * w[:, np.newaxis]\n",
        "            lf0_acc[start:end] += l * w[:, np.newaxis]\n",
        "            vuv_acc[start:end] += v * w[:, np.newaxis]\n",
        "            bap_acc[start:end] += b * w[:, np.newaxis]\n",
        "            weight[start:end]  += w\n",
        "            start += step\n",
        "\n",
        "    ws = np.maximum(weight, 1e-8)\n",
        "    return (mgc_acc / ws[:, None], lf0_acc / ws[:, None],\n",
        "            vuv_acc / ws[:, None], bap_acc / ws[:, None])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 6. メイン推論関数 (Gradio コールバック)\n",
        "# =============================================================================\n",
        "def run_inference(\n",
        "    singer_name, ust_file,\n",
        "    pitch_correction, gv_alpha, timbre_smooth, pitch_smooth,\n",
        "    formant_strength, breath_intensity,\n",
        "    use_multiband, volume_gain,\n",
        "    consonant_mode, consonant_length,\n",
        "    progress=gr.Progress(track_tqdm=True),\n",
        "):\n",
        "    logs = []\n",
        "    def log(msg): logs.append(msg); print(msg)\n",
        "\n",
        "    try:\n",
        "        if not singer_name or singer_name not in MODEL_REGISTRY:\n",
        "            raise ValueError(f\"歌手が選択されていません。MODEL_REGISTRY にモデルを登録してください。\")\n",
        "        if ust_file is None:\n",
        "            raise ValueError(\"UST ファイルがアップロードされていません。\")\n",
        "\n",
        "        yvs_path = MODEL_REGISTRY[singer_name]\n",
        "        if not os.path.exists(yvs_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"モデルファイルが見つかりません: {yvs_path}\\n\"\n",
        "                f\"Google Drive がマウントされているか確認してください。\"\n",
        "            )\n",
        "\n",
        "        ust_path = ust_file if isinstance(ust_file, str) else ust_file.name\n",
        "\n",
        "        log(f\"👤 歌手: {singer_name}\")\n",
        "        log(\"📥 モデル読み込み中...\")\n",
        "        progress(0.05, desc=\"モデル読み込み中...\")\n",
        "        model, sc, load_log = _load_model(yvs_path)\n",
        "        log(load_log)\n",
        "\n",
        "        log(\"🎼 UST 解析中...\")\n",
        "        progress(0.15, desc=\"UST 解析中...\")\n",
        "        notes, tempo = parse_ust(ust_path)\n",
        "        if not notes:\n",
        "            raise ValueError(\"UST ファイルから有効なノートが取得できませんでした。\")\n",
        "\n",
        "        log(\"📝 ノート処理中...\")\n",
        "        progress(0.25, desc=\"ノート処理中...\")\n",
        "        if consonant_mode == \"ON\":\n",
        "            log(f\"  子音固定モード: {consonant_length:.0f}ms\")\n",
        "            ids, nums, p_ids = ust_to_features_fixed_consonant(\n",
        "                notes, tempo, float(consonant_length))\n",
        "        else:\n",
        "            ids, nums, p_ids = ust_to_features(notes, tempo)\n",
        "\n",
        "        if ids.size == 0:\n",
        "            raise ValueError(\"UST から有効なノート情報が抽出できませんでした。\")\n",
        "\n",
        "        lf0_target  = nums[:, 2].copy()\n",
        "        nums_scaled = sc[\"nums\"].transform(nums)\n",
        "\n",
        "        log(\"🧠 推論実行中 (v9.0 高品質モード)...\")\n",
        "        progress(0.40, desc=\"AI 推論中...\")\n",
        "        m_mgc, m_lf0, m_vuv, m_bap = _inference_chunked(\n",
        "            model, ids, nums_scaled, p_ids, chunk_size=1500, overlap=300)\n",
        "\n",
        "        log(\"🔄 後処理中...\")\n",
        "        progress(0.65, desc=\"後処理中...\")\n",
        "\n",
        "        mgc = sc[\"mgc\"].inverse_transform(m_mgc)\n",
        "        bap = sc[\"bap\"].inverse_transform(m_bap)\n",
        "        lf0 = m_lf0.flatten()\n",
        "        vuv = m_vuv.flatten()\n",
        "\n",
        "        if pitch_correction > 0:\n",
        "            voiced = vuv > 0.5\n",
        "            if np.any(voiced):\n",
        "                n  = min(len(lf0_target), len(lf0))\n",
        "                vm = voiced[:n]\n",
        "                lf0[:n][vm] += (lf0_target[:n][vm] - lf0[:n][vm]) * pitch_correction\n",
        "\n",
        "        if \"gv_mgc\" in sc and gv_alpha > 0:\n",
        "            mgc = apply_gv(mgc, sc[\"gv_mgc\"][0], sc[\"gv_mgc\"][1], gv_alpha)\n",
        "\n",
        "        if timbre_smooth > 0:\n",
        "            for i in range(mgc.shape[1]):\n",
        "                mgc[:, i] = gaussian_filter1d(mgc[:, i], timbre_smooth)\n",
        "\n",
        "        lf0 = medfilt(lf0, kernel_size=3)\n",
        "        if pitch_smooth > 0:\n",
        "            lf0 = gaussian_filter1d(lf0, pitch_smooth)\n",
        "\n",
        "        for i in range(bap.shape[1]):\n",
        "            bap[:, i] = gaussian_filter1d(bap[:, i], 0.8)\n",
        "\n",
        "        f0 = np.exp(np.where(lf0 > 0, lf0, 0))\n",
        "        f0 = np.nan_to_num(f0, nan=0.0, posinf=1600.0, neginf=0.0)\n",
        "        f0 = np.clip(f0, 0, 1600.0)\n",
        "\n",
        "        vuv_smooth = gaussian_filter1d(vuv, 1.5)\n",
        "        f0[vuv_smooth < 0.5] = 0.0\n",
        "\n",
        "        log(\"🎵 レンダリング中 (WORLD 合成)...\")\n",
        "        progress(0.80, desc=\"WORLD 合成中...\")\n",
        "        y = synthesis_wav(\n",
        "            f0, mgc, bap, SAMPLE_RATE,\n",
        "            high_quality=True,\n",
        "            vuv=vuv_smooth,\n",
        "            formant_strength=float(formant_strength),\n",
        "            breath_intensity=float(breath_intensity),\n",
        "        )\n",
        "\n",
        "        if use_multiband:\n",
        "            log(\"  🎚️ マルチバンドコンプレッション適用中...\")\n",
        "            y    = multiband_compress(y, SAMPLE_RATE)\n",
        "            peak = np.max(np.abs(y))\n",
        "            if peak > 1e-6:\n",
        "                y = y / peak\n",
        "\n",
        "        y = y * float(volume_gain)\n",
        "        y = np.nan_to_num(y, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        y = soft_limiter(y, threshold=0.92)\n",
        "\n",
        "        progress(0.95, desc=\"WAV 書き出し中...\")\n",
        "        out_path = os.path.join(tempfile.gettempdir(), \"singing_output.wav\")\n",
        "        sf.write(out_path, y.astype(np.float32), SAMPLE_RATE)\n",
        "\n",
        "        log(f\"✅ 完了: {len(y) / SAMPLE_RATE:.2f} 秒の音声を生成しました\")\n",
        "        progress(1.0, desc=\"完了\")\n",
        "        return out_path, \"\\n\".join(logs)\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"❌ エラー: {e}\")\n",
        "        log(traceback.format_exc())\n",
        "        return None, \"\\n\".join(logs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Gradio UI\n",
        "# =============================================================================\n",
        "with gr.Blocks(title=\"yui2024式 歌唱合成ソフト v9.0\") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 🎤 yui2024式 歌唱合成ソフト v9.0\n",
        "        **Google Colab 対応 Gradio GUI — 完全単体版**\n",
        "        > 歌手を選択し、UST をアップロードして「歌声生成開始」を押してください。\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # ── 左カラム: 設定 ──────────────────────────────────────────────────\n",
        "        with gr.Column(scale=1):\n",
        "\n",
        "            gr.Markdown(\"### 📁 ファイル設定\")\n",
        "            singer_name = gr.Dropdown(\n",
        "                choices=_MODEL_CHOICES,\n",
        "                value=_MODEL_CHOICES[0] if _MODEL_CHOICES else None,\n",
        "                label=\"🎤 歌手を選択\",\n",
        "                info=\"MODEL_REGISTRY にモデルを登録すると選択肢が増えます\",\n",
        "            )\n",
        "            ust_file = gr.File(\n",
        "                label=\"UST ファイル (楽譜) をアップロード\",\n",
        "                file_types=[\".ust\"],\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ⚙️ 音質調整\")\n",
        "            pitch_correction = gr.Slider(0.0, 1.0,   value=1.0,    step=0.01,   label=\"🎯 ピッチ補正強度\")\n",
        "            gv_alpha         = gr.Slider(0.0, 2.0,   value=1.0,    step=0.01,   label=\"GV強度 (メリハリ)\")\n",
        "            timbre_smooth    = gr.Slider(0.0, 3.0,   value=0.0,    step=0.1,    label=\"🎵 音色滑らかさ (σ)\")\n",
        "            pitch_smooth     = gr.Slider(0.0, 3.0,   value=0.0,    step=0.1,    label=\"🎶 音程滑らかさ (σ)\")\n",
        "\n",
        "            gr.Markdown(\"### ✨ 高度な音質設定\")\n",
        "            formant_strength = gr.Slider(0.0, 2.0,   value=1.0,    step=0.01,   label=\"🔊 フォルマント強調\")\n",
        "            breath_intensity = gr.Slider(0.0, 0.004, value=0.0015, step=0.0001, label=\"💨 息継ぎ音の強さ\")\n",
        "\n",
        "            gr.Markdown(\"### 🎚️ ダイナミクス処理\")\n",
        "            use_multiband = gr.Checkbox(label=\"マルチバンドコンプレッション\", value=False)\n",
        "            volume_gain   = gr.Slider(0.5, 1.0, value=0.88, step=0.01, label=\"音量ゲイン\")\n",
        "\n",
        "            gr.Markdown(\"### 🎵 子音長制御\")\n",
        "            consonant_mode   = gr.Radio(choices=[\"ON\", \"OFF\"], value=\"ON\", label=\"子音長固定モード\")\n",
        "            consonant_length = gr.Slider(20, 150, value=65, step=1, label=\"子音長 (ms)\")\n",
        "\n",
        "            run_btn = gr.Button(\"🎵 歌声生成開始\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        # ── 右カラム: 出力 ──────────────────────────────────────────────────\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎧 生成結果\")\n",
        "            audio_out = gr.Audio(\n",
        "                label=\"生成音声（再生・ダウンロード）\",\n",
        "                type=\"filepath\",\n",
        "                interactive=False,\n",
        "            )\n",
        "            log_out = gr.Textbox(\n",
        "                label=\"ログ\",\n",
        "                lines=25,\n",
        "                max_lines=40,\n",
        "                interactive=False,\n",
        "                show_copy_button=True,\n",
        "            )\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=run_inference,\n",
        "        inputs=[\n",
        "            singer_name, ust_file,\n",
        "            pitch_correction, gv_alpha, timbre_smooth, pitch_smooth,\n",
        "            formant_strength, breath_intensity,\n",
        "            use_multiband, volume_gain,\n",
        "            consonant_mode, consonant_length,\n",
        "        ],\n",
        "        outputs=[audio_out, log_out],\n",
        "    )\n",
        "\n",
        "# =============================================================================\n",
        "# 8. 起動 (share=True で公開 URL を発行)\n",
        "# =============================================================================\n",
        "demo.launch(share=True, debug=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 デバイス: cuda\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b2638cfecae2a30ac5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b2638cfecae2a30ac5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "R8JCm9jOdrBl",
        "outputId": "f7a1410b-7309-4681-dcb8-be4c774b60cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7xcnUOoehjt",
        "outputId": "85b51416-a4af-410a-f67c-064bac7db5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyworld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX7LD8PnfLwP",
        "outputId": "f4c79be4-1e82-4eab-c8a6-b01f4b08490b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyworld\n",
            "  Downloading pyworld-0.3.5.tar.gz (261 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyworld) (2.0.2)\n",
            "Building wheels for collected packages: pyworld\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.5-cp312-cp312-linux_x86_64.whl size=943059 sha256=d7c6f5480701e62be1428d4de4fc34c1719fbcbd55dfe6acb4056573048481d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ac/58/c6a1791ec6d17f3a99b6ccdec92b472f560cb5c564b83dd77e\n",
            "Successfully built pyworld\n",
            "Installing collected packages: pyworld\n",
            "Successfully installed pyworld-0.3.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
